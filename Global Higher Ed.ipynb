{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bita00c945b3a3448399f7aff96df15e44a",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "#Goal: to practice using Python and Jupyter Notebooks to analyze text and then visualize the results. \n",
    "\n",
    "#For this crude project, I will look policy texts and see how their explicit use of key words of globalization, neoliberalism, and human capital theory change over time. I will be first define the key words I am looking for, then I will iterate over the policy text to count the frequency of the times these words are used. The policy documents will then be compared based on year published and the policy writing institution."
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label and define key words\n",
    "\n",
    "#Ideas and buzzwords were taken from skimming some articles on the topics. There isn't a great reason theoretical for choosing these specific articles, instead they serve as a quick guide to get a crude list of words for the main goal of improving coding and textual analysis. \n",
    "\n",
    "# Human Capital Theory: Holden and Biddle (2017), \"The Introduction of Human Capital Theory into Education Policy in the United States\", https://doi.org/10.1215/00182702-4296305\n",
    "# Internationalization : Knight (1994), Internationalization-Elements and Checkpoints\n",
    "\n",
    "human_capital_theory = [\"economic growth\", \"economy\", \"economic\" \"poverty\", \"investment\", \"human capital\", \"capital\", \"unemployment\", \"employment\", \"jobs\", \"development\", \"careers\"]\n",
    "\n",
    "internationalization = [\"international\", \"internationalization\", \"internationalisation\", \"internationalized\" \"global\", \"globalism\", \"globalization\", \"globalisation\", \"foreign\", \"cross-cultural\",  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries and Defining core functions, \n",
    "\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def count_words (text):\n",
    "    \"\"\"\n",
    "    Takes a text and returns a dict with word + number of times used\n",
    "    \"\"\"\n",
    "    word_count = {}\n",
    "    text = text.lower()\n",
    "    skip_punctuation = [\",\", \".\", \"''\", '\"\"',\";\", \":\", \"-\", \"?\", \"!\"]\n",
    "    for char in skip_punctuation:\n",
    "        text = text.replace(char,\"\")\n",
    "    for word in text.split(\" \"):\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    "    return word_count\n",
    "\n",
    "def python_counter(text):\n",
    "    \"\"\"\n",
    "    Create a counting dictionary of unique words and word counts, from an inputted text (that removes puncuation and lowercases)\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    skip_punctuation = [\",\", \".\", \"'\", '\"',\";\", \":\"]\n",
    "    for char in skip_punctuation:\n",
    "        text = text.replace(char,\"\")\n",
    "    word_counts = Counter(text.split(\" \"))\n",
    "    return word_counts\n",
    "\n",
    "def read_book(title_path):\n",
    "    \"\"\"\n",
    "    Read a book and return it as a string.\n",
    "    \"\"\"\n",
    "    with open(title_path, \"r\", encoding=\"utf8\") as current_file:\n",
    "        text = current_file.read()\n",
    "        text = text.replace(\"\\n\", \"\").replace(\"\\r\",\"\")\n",
    "    return text\n",
    "\n",
    "def word_stats(python_counter):\n",
    "    num_unique = len(python_counter)\n",
    "    counts = python_counter.values()\n",
    "    return (num_unique, counts)\n",
    "\n",
    "def wordgroup_stats(python_counter, wordlist):\n",
    "    \"\"\"\n",
    "    Takes dic of unique words and count, read through pythoncounter, and a wordlist.\n",
    "    Outputs a dictionary of matched words and their count.\n",
    "    \"\"\"\n",
    "    unique_textwords = python_counter\n",
    "    matchedwords = {}\n",
    "    for word, value in unique_textwords.items():\n",
    "        if word in wordlist:\n",
    "            matchedwords[word] = value\n",
    "    no_of_matched = len(matchedwords.keys())\n",
    "    return matchedwords, no_of_matched\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts to upload and use\n",
    "ADBtextpaths = [\"/Users/samschmoker/Desktop/Coding/Projects/MasterPython/DataPractice/Policy/ADB/ADBpdftotext/ADB Education Policy 2002.txt\", \"/Users/samschmoker/Desktop/Coding/Projects/MasterPython/DataPractice/Policy/ADB/ADBpdftotext/ADB, 2008-Education is a Core Operation Area of ADB.txt\", \"/Users/samschmoker/Desktop/Coding/Projects/MasterPython/DataPractice/Policy/ADB/ADBpdftotext/ADB, 2013a-ADB’s Support for Achieving the Millennium Development Goals.txt\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "#Lets see what words are commonly used in all documents\n",
    "\n",
    "wordsindocs = {}\n",
    "wordsindoc1 = {}\n",
    "wordsindoc2 = {}\n",
    "wordsinalldocs= {}\n",
    "for document in ADBtextpaths[0:1]:\n",
    "    title = document.rsplit(\"/\", 1)[-1]\n",
    "    text = read_book(document)\n",
    "    wordsindoc1 = count_words(text)\n",
    "\n",
    "for document in ADBtextpaths[1:2]:\n",
    "    title = document.rsplit(\"/\", 1)[-1]\n",
    "    text = read_book(document)\n",
    "    wordsindoc2 = count_words(text)\n",
    "    for word in wordsindoc1.keys():\n",
    "        if word in wordsindoc2.keys():\n",
    "            wordsinalldocs[word] = 1\n",
    " \n",
    "for document in ADBtextpaths[2:]:\n",
    "    title = document.rsplit(\"/\", 1)[-1]\n",
    "    text = read_book(document)\n",
    "    wordsinthisdoc = count_words(text)\n",
    "    for word in wordsinalldocs.copy():\n",
    "        if word in wordsinthisdoc:\n",
    "            wordsinalldocs[word] += 1\n",
    "        else:\n",
    "            del wordsinalldocs[word]\n",
    "\n",
    "for word in wordsinalldocs.copy():\n",
    "    if len(word) < 4:\n",
    "        del wordsinalldocs[word]\n",
    "\n",
    "wordsgreaterthan4 = sorted (list(wordsinalldocs.keys()))\n",
    "\n",
    "\n",
    "print (len(wordsgreaterthan4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "170\n"
     ]
    }
   ],
   "source": [
    "#Clean it!\n",
    "#Inspect the Word list for words greater than 4. Perhaps there are some generic words that should be taken out. \n",
    "\n",
    "#print (wordsgreaterthan4)\n",
    "\n",
    "wordsto_remove = [\"adb's'\", \"areas\", \"beyond\", \"billion\", \"despite\", \"numbers\", \"about\", \"also\",\"basic\", \"become\", \"every\", \"forthe\", \"from\", \"have\", \"high\", \"higher\",\"important\", \"given\", \"good\", \"includes\", \"inthe\", \"less\", \"levelsof\", \"made\", \"many\", \"meet\", \"ofadb\", \"(adb)\",\"other\", \"plays\", \"plan\", \"plans\", \"refers\", \"reflect\",\"response\", \"role\",\"strong\",\"than\", \"that\", \"their\", \"this\", \"total\", \"tothe\", \"uses\", \"well\", \"will\", \"while\", \"with\", \"further\", \"example\", \"especially\"]\n",
    "\n",
    "edited_wordsinalldoc = []\n",
    "for word in wordsgreaterthan4:\n",
    "    if word not in wordsto_remove:\n",
    "        edited_wordsinalldoc.append(word)\n",
    "\n",
    "print (len(edited_wordsinalldoc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                           Doc Title Unique_Words Total_Words  \\\n1                      ADB Education Policy 2002.txt         3934       17861   \n2  ADB, 2008-Education is a Core Operation Area o...          887        1511   \n3  ADB, 2013a-ADB’s Support for Achieving the Mil...         7518       38587   \n\n  No. of Matched                                              words  \\\n1           2844  {'framework': 26, 'policy': 115, 'asian': 17, ...   \n2            381  {'education': 46, 'core': 2, 'operation': 2, '...   \n3           4311  {'support': 388, 'development': 296, 'knowledg...   \n\n   % matched to total  \n1           15.922961  \n2           25.215089  \n3           11.172156  \n"
     ]
    }
   ],
   "source": [
    "#Now compare each doc to the overall wordmatching list\n",
    "\n",
    "table = pd.DataFrame(columns=(\"Doc Title\", \"Unique_Words\", \"Total_Words\", \"No. of Matched\",\"words\", \"% matched to total\"))\n",
    "i = 1\n",
    "file_dict = {}\n",
    "for document in ADBtextpaths:\n",
    "    title = document.rsplit(\"/\", 1)[-1]\n",
    "    text = read_book(document)\n",
    "    (num_unique, counts) = word_stats(count_words(text))    \n",
    "    matchedwords, no_of_matched = wordgroup_stats(count_words(text), edited_wordsinalldoc)\n",
    "    percent_match = (sum(matchedwords.values()) / sum(counts)) * 100\n",
    "\n",
    "    #input text stats into table      \n",
    "    table.loc[i] = title, num_unique, sum(counts), sum(matchedwords.values()), matchedwords, percent_match\n",
    "    i += 1\n",
    "\n",
    "print (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['adb’s', 'aims', 'andvocational', 'assessment', 'assistance', 'bank', 'basiceducation', 'capacity', 'community', 'comprehensive', 'cost', 'decentralized', 'democratic', 'development', 'developmentadb', 'developmentand', 'dmcs', 'economic', 'education', 'effective', 'efficiency', 'equity', 'expansion', 'female', 'financial', 'framework', 'generation', 'government', 'grants', 'information', 'infrastructure', 'initiatives', 'innovation', 'institutions', 'knowledge', 'labor', 'lessons', 'loans', 'longterm', 'market', 'operation', 'operations', 'opportunities', 'participation', 'partnerships', 'past', 'people', 'people’s', 'poor', 'portfolio', 'poverty', 'private', 'programs', 'progress', 'project', 'projects', 'public', 'quality', 'region’s', 'relevance', 'remote', 'secondary', 'secondaryeducation', 'sector', 'service', 'services', 'share', 'skills', 'social', 'stakeholders', 'strategy', 'subsectors', 'substantial', 'substantially', 'success', 'successfully', 'sustainable', 'system', 'systems', 'technical', 'technology', 'training', 'university', 'vocational', 'workers', 'workforce']\n"
     ]
    }
   ],
   "source": [
    "#Analyze and categorize \n",
    "\n",
    "#Look at edited_wordinalldoc list. Then manually sort into sub-lists\n",
    "\n",
    "place_words = [\"asia\", \"asian\", \"bangladesh\", \"cambodia\", \"china\", \"east\", \"indonesia\", \"kyrgyz\", \"manila\", \"nepal\", \"pacific\", \"pakistan\",\"republic\", \"rural\",\"south\", \"southeast\",\"theasian\", \"viet\", \"countries\", \"region\", \"region's\", \"regional\", \"national\"]\n",
    "\n",
    "verbs = [\"access\", \"collaboration\", \"contribute\", \"contribution\", \"cooperation\", \"delivery\", \"develop\", \"developing\", \"enroll\", \"ensuring\", \"expand\", \"expanding\", \"facilitate\", \"financed\", \"financing\", \"focus\", \"growing\", \"guarantees\", \"help\", \"helped\", \"helping\", \"impact\", \"improve\", \"improved\", \"improving\", \"increase\", \"increasing\", \"increasingly\", \"leverage\", \"monitoring\", \"need\", \"needs\", \"performance\", \"practice\", \"provide\", \"provides\", \"providing\", \"pursue\", \"reduce\", \"reduced\", \"reflects\", \"sharing\", \"strengthening\", \"support\", \"supports\", \"supported\", \"supporting\", \"toimprove\", \"study\"]\n",
    "\n",
    "misc = [\"alone\", \"especially\", \"example\", \"further\", \"decades\", \"individual\", \"million\", \"publication\", \"policy\", \"significant\", \"specific\", \"studies\", \"through\", \"critical\", \"core\", \"completion\"]\n",
    "\n",
    "other = []\n",
    "\n",
    "for word in edited_wordsinalldoc:\n",
    "    if word not in place_words and word not in verbs and word not in misc:\n",
    "        other.append(word)\n",
    "\n",
    "print (other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick Recapping thoughts:\n",
    "\n",
    "#This was a pretty quick play-around that practiced some different things in Pythong:\n",
    "    #Practiced looping over .txts in a folder (so really easy to scale up with a lot more documents!)\n",
    "\n",
    "    #First, I practiced searching the texts for specific words I thought of apriori of reading the documents. \n",
    "#this was just a fun hunch that didn't end up showing anything useful in this case. I think I could improve on this in two ways: first by first reading through a subset of the texts to get more familiar with the language used and so what words might crop up and second by looping through a much larger set of texts (ideally to show how the use of certain words increased/decreased over time). Overall, the Python part worked fine, I just didn't put enough time into the specific question to make it worth while. \n",
    "\n",
    "    #Second, I looped over the texts to find all the words present in all texts. This was kind of cool and useful,though the second document had considerably less words than the other two so most likely considerably skewed the word list to this document. I think if I was to scale this with more documents I wouldn't look at words that matched 100% to all documents. Instead I would set up a Python dictionary who's values would count how many documents the words are in. That would mean it would be easy to see that, for example, the word \"rural\" was present in 75% of documents looked at. \n",
    "\n",
    "    #After finding all of the similiar words in the texts the next step is to display these in a clear way. I am not doing that now because that wasn't the real goal of this practice. The results that I did find--most easily seen in the last lists of categorized words--have some interesting tidbits. Notably the emphasis on certain countries in ASEAN, and China, without mentioning others. Mentioning Manila makes sense since that is the homebase of ADB, and China is obviously a huge player. Curious that other powers, like Japan, SK, Thailand and others are mentioned. Also it doesn't seem that other international institutions are included--but maybe that is a formatting issue that should be looked at more closely. \n",
    "\n",
    "    #Just skimming through this word list at the end is a better indicator of my original goal of assuming certain neoliberal/human capital word use. Work, skills, development, infastructure, poverty, private are all seen in all documents and all (most likely) represent a human capital justification of the education policies. Clearly I would need to dive more into each document and loop through more documents to get a clearer picture. "
   ]
  }
 ]
}